{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second In-class-exercise (09/22/2022, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDomain problem: \\n. My domain problem is to analyse the price data that obtained from the ebay of colection og women tops.\\n\\nResearch question:\\n1. How does the Internet affect the way people think and learn?\\n\\n2. How does social media use affect mental health?\\n\\n3. How does the use of technology affect the way we interact with others?\\n\\n4. What are the effects of digital detoxing?\\n\\nTo answer these questions, data should be collected on the following:\\n1. How often people use the Internet and for what purposes.\\n2. How often people use social media and for what purposes.\\n3. How often people use technology and for what purposes.\\n4. How often people engage in digital detoxing and for what purposes.\\n. At what price would you begin to think the {product/service} is so inexpensive \\n\\nData Required to answer the questions:\\nThe following are the attributes of the required data to answer the research question\\n. price of the product\\n\\nWe can gather 20 page samples from the following link: https://www.amazon.com/\\n\\n\\nSteps for collecting and saving the data:\\n. We use web scraping technique to scrape the data.\\n. We use BeautifulSoup library to parse the Densho Digital Repository.\\n. There are 1000 pages of women tops.\\n. We parse through each page and go to the women tops link.\\n. We parse the Segment Body-> span elements in the html code and then store the text data in a dictionary format.\\n. The dictionary is the prepared data for our research.\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Domain problem: \n",
    ". My domain problem is to analyse the price data that obtained from the ebay of colection og women tops.\n",
    "\n",
    "Research question:\n",
    "1. How does the Internet affect the way people think and learn?\n",
    "\n",
    "2. How does social media use affect mental health?\n",
    "\n",
    "3. How does the use of technology affect the way we interact with others?\n",
    "\n",
    "4. What are the effects of digital detoxing?\n",
    "\n",
    "To answer these questions, data should be collected on the following:\n",
    "1. How often people use the Internet and for what purposes.\n",
    "2. How often people use social media and for what purposes.\n",
    "3. How often people use technology and for what purposes.\n",
    "4. How often people engage in digital detoxing and for what purposes.\n",
    ". At what price would you begin to think the {product/service} is so inexpensive \n",
    "\n",
    "Data Required to answer the questions:\n",
    "The following are the attributes of the required data to answer the research question\n",
    ". price of the product\n",
    "\n",
    "We can gather 20 page samples from the following link: https://www.amazon.com/\n",
    "\n",
    "\n",
    "Steps for collecting and saving the data:\n",
    ". We use web scraping technique to scrape the data.\n",
    ". We use BeautifulSoup library to parse the Densho Digital Repository.\n",
    ". There are 1000 pages of women tops.\n",
    ". We parse through each page and go to the women tops link.\n",
    ". We parse the Segment Body-> span elements in the html code and then store the text data in a dictionary format.\n",
    ". The dictionary is the prepared data for our research.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Price\n",
      "0    350.\n",
      "1    220.\n",
      "2    280.\n",
      "3    142.\n",
      "4    295.\n",
      "..    ...\n",
      "907  162.\n",
      "908  225.\n",
      "909  450.\n",
      "910  595.\n",
      "911  485.\n",
      "\n",
      "[912 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "# importing the libraries...\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# url of the densho digital repository to scrape the data\n",
    "main_url = \"https://www.amazon.com/s?i=fashion&bbn=23370962011&rh=n%3A7141123011%2Cn%3A23370962011%2Cn%3A7147440011%2Cn%3A1040660%2Cn%3A1045024%2Cp_85%3A2470955011&s=date-desc-rank&dc&ds=v1%3ApftFVJZe9iCRK6ECM1qkAx5HsC8yTxqkt22TjenEbSw&content-id=amzn1.sym.353f2924-e405-4b4f-bf96-d5b798e102ba%3Aamzn1.sym.353f2924-e405-4b4f-bf96-d5b798e102ba&pd_rd_r=fc410f66-b8f1-4900-a1e1-6eaf7e395a48&pd_rd_w=CUr7E&pd_rd_wg=UsmKm&pf_rd_i=7147440011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=353f2924-e405-4b4f-bf96-d5b798e102ba&pf_rd_r=15QAGSSYGH2302FRNQPY&pf_rd_s=merchandised-search-{}&pf_rd_t=101&qid=1664146062&rnid=1040660&wi=l61aaov1_0&ref=slsr_premiumfall2022category_d_t_m_vn_l61aaov1_0\"\n",
    "df = pd.DataFrame()\n",
    "price_list_dict = {\"Price\": []}\n",
    "for page_num in range(1, 20):\n",
    "    page_num = page_num + 9\n",
    "    # we create the data soup of the main url page html elements.\n",
    "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    url1 = urlopen(link1)\n",
    "    data1 = url1.read()\n",
    "    data1_soup = BeautifulSoup(data1)\n",
    "    # print(data1_soup)\n",
    "    \n",
    "    prices = data1_soup.find_all(\"span\", {\"class\": \"a-price-whole\"})\n",
    "    for price in prices:\n",
    "        price_list_dict['Price'].append(price.text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(price_list_dict)    \n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
    "\n",
    "The following information of the article needs to be collected:\n",
    "\n",
    "(1) Title\n",
    "\n",
    "(2) Venue/journal/conference being published\n",
    "\n",
    "(3) Year\n",
    "\n",
    "(4) Authors\n",
    "\n",
    "(5) Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Titles  \\\n",
      "0          [BOOK][B] Information retrieval interaction   \n",
      "1    [PDF][PDF] Evaluation of evaluation in informa...   \n",
      "2                                Information retrieval   \n",
      "3           [CITATION][C] Modern information retrieval   \n",
      "4    A definition of relevance for information retr...   \n",
      "..                                                 ...   \n",
      "185  Adarank: a boosting algorithm for information ...   \n",
      "186    [HTML][HTML] Geographical information retrieval   \n",
      "187             Terrier information retrieval platform   \n",
      "188  [BOOK][B] Information retrieval: data structur...   \n",
      "189                  Distributed information retrieval   \n",
      "\n",
      "                                       Author_and_Year  \\\n",
      "0                 P Ingwersen - 1992 - oarklibrary.com   \n",
      "1    T Saracevic - … on Research and development in...   \n",
      "2            Q Mei, DR Radev - 2014 - academic.oup.com   \n",
      "3          BY Ricardo - 1999 - Pearson Education India   \n",
      "4    WS Cooper - Information storage and retrieval,...   \n",
      "..                                                 ...   \n",
      "185  J Xu, H Li - … on Research and development in ...   \n",
      "186  CB Jones, RS Purves - … Journal of Geographica...   \n",
      "187  I Ounis, G Amati, V Plachouras, B He… - … on I...   \n",
      "188       WB Frakes, R Baeza-Yates - 1992 - dl.acm.org   \n",
      "189  J Callan - Advances in information retrieval, ...   \n",
      "\n",
      "                                              Abstract  \n",
      "0    … Information retrieval covers the problems re...  \n",
      "1    ABSTRACT Evaluation is a major force in resear...  \n",
      "2    Information Retrieval | The Oxford Handbook of...  \n",
      "3                                                       \n",
      "4    … The purpose of this paper, then, is to propo...  \n",
      "..                                                 ...  \n",
      "185  In this paper we address the issue of learning...  \n",
      "186  … geographic relevance and the non‐retrieval o...  \n",
      "187  Terrier is a modular platform for the rapid de...  \n",
      "188  … covering the major information retrieval alg...  \n",
      "189  A multi-database model of distributed informat...  \n",
      "\n",
      "[190 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "# importing the libraries...\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# url of the densho digital repository to scrape the data\n",
    "main_url = \"https://scholar.google.com/scholar?start={}&q=information+retrieval&hl=en&as_sdt=0,44\"\n",
    "df = pd.DataFrame()\n",
    "res_dict = {\"Titles\": [], \"Author_and_Year\": [], \"Abstract\": []}\n",
    "for page_num in range(1, 20):\n",
    "    # we create the data soup of the main url page html elements.\n",
    "    page_num=page_num + 9\n",
    "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    url1 = urlopen(link1)\n",
    "    data1 = url1.read()\n",
    "    data1_soup = BeautifulSoup(data1)\n",
    "    \n",
    "    title_list = data1_soup.find_all(\"h3\", {\"class\": \"gs_rt\"})\n",
    "    author_year_list = data1_soup.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "    abstract_list = data1_soup.find_all(\"div\", {\"class\": \"gs_rs\"})\n",
    "    \n",
    "    for title in title_list:\n",
    "        res_dict['Titles'].append(title.text)\n",
    "    for author_year in author_year_list:\n",
    "        res_dict['Author_and_Year'].append(author_year.text)\n",
    "    for abstract in abstract_list:\n",
    "        res_dict['Abstract'].append(abstract.text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(res_dict)    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
    "\n",
    "The following information needs to be collected:\n",
    "\n",
    "(1) User_name\n",
    "\n",
    "(2) Posted time\n",
    "\n",
    "(3) Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.10.1-py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.27.0 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from tweepy) (2.27.1)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib<2,>=1.2.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.1 requests-oauthlib-1.3.1 tweepy-4.10.1\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "!pip install tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sadiamahbub/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_key =\"2Em7SxlX9jPMfL4x97r3zMO0x\"\n",
    "customer_secret = \"sVbJzekKuiAgq83Y7gCwNVbSowqQokGVzWexKHl2cXIPceWtSd\"\n",
    "access_token =\"1439767876962029572-uUMt8oWRyzj9ilE5zk4uYbL93sCMPT\"\n",
    "access_token_secret = \"oydIGymn9bS767FVEMawE9GyGAnMmBJfaY2XXKmHnmliF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(customer_key, customer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter user name\tsmmohua\n"
     ]
    }
   ],
   "source": [
    " username =str(input(\"Enter user name\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
